{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navigation\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the first project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893).\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing some necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "import operator\n",
    "import tensorflow as tf\n",
    "from tensorflow.python import debug as tf_debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Banana.app\"`\n",
    "- **Windows** (x86): `\"path/to/Banana_Windows_x86/Banana.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Banana_Windows_x86_64/Banana.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Banana_Linux/Banana.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Banana_Linux/Banana.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Banana_Linux_NoVis/Banana.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Banana_Linux_NoVis/Banana.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Banana.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Banana.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "I0909 19:17:47.966329 140205682263872 environment.py:105] \n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"Banana_Linux/Banana.x86_64\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "The simulation contains a single agent that navigates a large environment.  At each time step, it has four actions at its disposal:\n",
    "- `0` - walk forward \n",
    "- `1` - walk backward\n",
    "- `2` - turn left\n",
    "- `3` - turn right\n",
    "\n",
    "The state space has `37` dimensions and contains the agent's velocity, along with ray-based perception of objects around agent's forward direction.  A reward of `+1` is provided for collecting a yellow banana, and a reward of `-1` is provided for collecting a blue banana. \n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 1\n",
      "Number of actions: 4\n",
      "States look like: [1.         0.         0.         0.         0.84408134 0.\n",
      " 0.         1.         0.         0.0748472  0.         1.\n",
      " 0.         0.         0.25755    1.         0.         0.\n",
      " 0.         0.74177343 0.         1.         0.         0.\n",
      " 0.25854847 0.         0.         1.         0.         0.09355672\n",
      " 0.         1.         0.         0.         0.31969345 0.\n",
      " 0.        ]\n",
      "States have length: 37\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents in the environment\n",
    "print('Number of agents:', len(env_info.agents))\n",
    "\n",
    "# number of actions\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Number of actions:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "state = env_info.vector_observations[0]\n",
    "print('States look like:', state)\n",
    "state_size = len(state)\n",
    "print('States have length:', state_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Once this cell is executed, you will watch the agent's performance, if it selects an action (uniformly) at random with each time step.  A window should pop up that allows you to observe the agent, as it moves through the environment.  \n",
    "\n",
    "Of course, as part of the project, you'll have to change the code so that the agent is able to use its experience to gradually choose better actions when interacting with the environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=False)[brain_name] # reset the environment\n",
    "state = env_info.vector_observations[0]            # get the current state\n",
    "score = 0                                          # initialize the score\n",
    "while True:\n",
    "    action = np.random.randint(action_size)        # select an action\n",
    "    env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "    next_state = env_info.vector_observations[0]   # get the next state\n",
    "    reward = env_info.rewards[0]                   # get the reward\n",
    "    done = env_info.local_done[0]                  # see if episode has finished\n",
    "    score += reward                                # update the score\n",
    "    state = next_state                             # roll over the state to next time step\n",
    "    if done:                                       # exit loop if episode finished\n",
    "        break\n",
    "    \n",
    "print(\"Score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork:\n",
    "\n",
    "    def __init__(self, state_size, action_size, learning_rate, name):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.name = name\n",
    "\n",
    "        with tf.variable_scope(self.name):\n",
    "            self.inputs_ = tf.placeholder(tf.float32, [None, *state_size], name='inputs')\n",
    "            self.IS_weights_ = tf.placeholder(tf.float32, [None, 1], name='IS_weights')\n",
    "            self.actions_ = tf.placeholder(tf.float32, [None, action_size], name='actions')\n",
    "            self.target_Q = tf.placeholder(tf.float32, [None], name='target_Q')\n",
    "\n",
    "            self.value_fc = tf.layers.dense(\n",
    "                inputs=self.inputs_,\n",
    "                units=512,\n",
    "                activation=tf.nn.elu,\n",
    "                kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                name='value_fc'\n",
    "            )\n",
    "\n",
    "            self.value = tf.layers.dense(\n",
    "                inputs=self.value_fc,\n",
    "                units=1,\n",
    "                activation=None,\n",
    "                kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                name='value'\n",
    "            )\n",
    "\n",
    "            self.advantage_fc = tf.layers.dense(\n",
    "                inputs=self.inputs_,\n",
    "                units=512,\n",
    "                activation=tf.nn.elu,\n",
    "                kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                name='advantage_fc'\n",
    "            )\n",
    "\n",
    "            self.advantage = tf.layers.dense(\n",
    "                inputs=self.advantage_fc,\n",
    "                units=action_size,\n",
    "                activation=None,\n",
    "                kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                name='advantage'\n",
    "            )\n",
    "\n",
    "            self.output = self.value + tf.subtract(\n",
    "                self.advantage,\n",
    "                tf.reduce_mean(self.advantage, axis=1, keepdims=True))\n",
    "            self.Q = tf.reduce_sum(tf.multiply(self.output, self.actions_), axis=1)\n",
    "            self.absolute_errors = tf.abs(self.target_Q - self.Q)\n",
    "            self.loss = tf.reduce_mean(self.IS_weights_ * tf.squared_difference(self.target_Q, self.Q))\n",
    "            self.optimizer = tf.train.RMSPropOptimizer(self.learning_rate).minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, size):\n",
    "        assert size > 0\n",
    "        self._storage = []\n",
    "        self._maxsize = size\n",
    "        self._next_idx = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._storage)\n",
    "\n",
    "    def add(self, obs_t, action, reward, obs_tp1, done):\n",
    "        data = (obs_t, action, reward, obs_tp1, done)\n",
    "\n",
    "        if self._next_idx >= len(self._storage):\n",
    "            self._storage.append(data)\n",
    "        else:\n",
    "            self._storage[self._next_idx] = data\n",
    "        self._next_idx += 1\n",
    "        if self._next_idx == self._maxsize:\n",
    "            self._next_idx = 0\n",
    "\n",
    "    def _encode_sample(self, idxes):\n",
    "        obses_t, actions, rewards, obses_tp1, dones = [], [], [], [], []\n",
    "        for i in idxes:\n",
    "            data = self._storage[i]\n",
    "            obs_t, action, reward, obs_tp1, done = data\n",
    "            obses_t.append(np.array(obs_t, copy=False))\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            obses_tp1.append(np.array(obs_tp1, copy=False))\n",
    "            dones.append(done)\n",
    "        return np.array(obses_t), np.array(actions), np.array(rewards), np.array(obses_tp1), np.array(dones)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        n = len(self._storage)\n",
    "        idxes = [np.random.randint(n) for _ in range(batch_size)]\n",
    "        return self._encode_sample(idxes)\n",
    "\n",
    "\n",
    "class PrioritizedReplayBuffer(ReplayBuffer):\n",
    "    def __init__(self, size, alpha):\n",
    "        super(PrioritizedReplayBuffer, self).__init__(size)\n",
    "        assert alpha >= 0\n",
    "        self._alpha = alpha\n",
    "\n",
    "        it_capacity = 1\n",
    "        while it_capacity < size:\n",
    "            it_capacity *= 2\n",
    "\n",
    "        self._it_sum = SumSegmentTree(it_capacity)\n",
    "        self._it_min = MinSegmentTree(it_capacity)\n",
    "        self._it_max = MaxSegmentTree(it_capacity)\n",
    "\n",
    "    def add(self, *args, **kwargs):\n",
    "        idx = self._next_idx\n",
    "        super(PrioritizedReplayBuffer, self).add(*args, **kwargs)\n",
    "\n",
    "        max_priority = self._it_max.max()\n",
    "        if max_priority <= 0:\n",
    "            max_priority = 1.0\n",
    "\n",
    "        self._it_sum[idx] = self._it_min[idx] = self._it_max[idx] = max_priority\n",
    "\n",
    "    def _sample_proportional(self, batch_size):\n",
    "        res = []\n",
    "        p_total = self._it_sum.sum()\n",
    "        every_range_len = p_total / batch_size\n",
    "        for i in range(batch_size):\n",
    "            mass = np.random.rand() * every_range_len + i * every_range_len\n",
    "            idx = self._it_sum.find_prefixsum_idx(mass)\n",
    "            res.append(idx)\n",
    "        return res\n",
    "\n",
    "    def sample(self, batch_size, beta):\n",
    "        assert beta > 0\n",
    "\n",
    "        idxes = self._sample_proportional(batch_size)\n",
    "\n",
    "        weights = []\n",
    "        p_sum = self._it_sum.sum()\n",
    "        p_min = self._it_min.min() / p_sum\n",
    "        n = len(self._storage)\n",
    "        max_weight = (p_min * n) ** (-beta)\n",
    "\n",
    "        for idx in idxes:\n",
    "            p_sample = self._it_sum[idx] / p_sum\n",
    "            weight = (p_sample * n) ** (-beta)\n",
    "            weights.append(weight / max_weight)\n",
    "        weights = np.array(weights)\n",
    "        encoded_sample = self._encode_sample(idxes)\n",
    "        return tuple(list(encoded_sample) + [weights, idxes])\n",
    "\n",
    "    def update_priorities(self, idxes, priorities):\n",
    "        assert len(idxes) == len(priorities)\n",
    "        n = len(self._storage)\n",
    "        for idx, priority in zip(idxes, priorities):\n",
    "            priority += 0.01\n",
    "            assert priority > 0\n",
    "            assert 0 <= idx < n\n",
    "            self._it_sum[idx] = \\\n",
    "                self._it_min[idx] = \\\n",
    "                self._it_max[idx] = min(1.0, priority) ** self._alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentTree(object):\n",
    "    def __init__(self, capacity, operation, neutral_element):\n",
    "        assert capacity > 0 and capacity & (capacity - 1) == 0, \"capacity must be positive and a power of 2\"\n",
    "        self._capacity = capacity\n",
    "        self._value = [neutral_element for _ in range(2 * capacity)]\n",
    "        self._operation = operation\n",
    "\n",
    "    def _reduce_helper(self, start, end, node, node_start, node_end):\n",
    "        if start == node_start and end == node_end:\n",
    "            return self._value[node]\n",
    "        mid = (node_start + node_end) // 2\n",
    "        if end <= mid:\n",
    "            return self._reduce_helper(start, end, 2 * node, node_start, mid)\n",
    "        elif mid + 1 <= start:\n",
    "            return self._reduce_helper(start, end, 2 * node + 1, mid + 1, node_end)\n",
    "        else:\n",
    "            return self._operation(\n",
    "                self._reduce_helper(start, mid, 2 * node, node_start, mid),\n",
    "                self._reduce_helper(mid + 1, end, 2 * node + 1, mid + 1, node_end)\n",
    "            )\n",
    "\n",
    "    def reduce(self, start=0, end=None):\n",
    "        if end is None:\n",
    "            end = self._capacity\n",
    "        if end < 0:\n",
    "            end += self._capacity\n",
    "        end -= 1\n",
    "        return self._reduce_helper(start, end, 1, 0, self._capacity - 1)\n",
    "\n",
    "    def __setitem__(self, idx, val):\n",
    "        idx += self._capacity\n",
    "        self._value[idx] = val\n",
    "        idx //= 2\n",
    "        while idx >= 1:\n",
    "            self._value[idx] = self._operation(\n",
    "                self._value[2 * idx],\n",
    "                self._value[2 * idx + 1]\n",
    "            )\n",
    "            idx //= 2\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        assert 0 <= idx < self._capacity\n",
    "        return self._value[self._capacity + idx]\n",
    "\n",
    "\n",
    "class SumSegmentTree(SegmentTree):\n",
    "    def __init__(self, capacity):\n",
    "        super(SumSegmentTree, self).__init__(\n",
    "            capacity=capacity,\n",
    "            operation=operator.add,\n",
    "            neutral_element=0.0\n",
    "        )\n",
    "\n",
    "    def sum(self, start=0, end=None):\n",
    "        return super(SumSegmentTree, self).reduce(start, end)\n",
    "\n",
    "    def find_prefixsum_idx(self, prefixsum):\n",
    "        assert 0 <= prefixsum <= self.sum() + 1e-5\n",
    "        idx = 1\n",
    "        while idx < self._capacity:\n",
    "            if self._value[2 * idx] > prefixsum:\n",
    "                idx = 2 * idx\n",
    "            else:\n",
    "                prefixsum -= self._value[2 * idx]\n",
    "                idx = 2 * idx + 1\n",
    "        return idx - self._capacity\n",
    "\n",
    "    def __setitem__(self, idx, val):\n",
    "        assert val >= 0\n",
    "        super(SumSegmentTree, self).__setitem__(idx, val)\n",
    "\n",
    "\n",
    "class MinSegmentTree(SegmentTree):\n",
    "    def __init__(self, capacity):\n",
    "        super(MinSegmentTree, self).__init__(\n",
    "            capacity=capacity,\n",
    "            operation=min,\n",
    "            neutral_element=float('inf')\n",
    "        )\n",
    "\n",
    "    def min(self, start=0, end=None):\n",
    "        return super(MinSegmentTree, self).reduce(start, end)\n",
    "\n",
    "\n",
    "class MaxSegmentTree(SegmentTree):\n",
    "    def __init__(self, capacity):\n",
    "        super(MaxSegmentTree, self).__init__(\n",
    "            capacity=capacity,\n",
    "            operation=max,\n",
    "            neutral_element=float('-inf')\n",
    "        )\n",
    "\n",
    "    def max(self, start=0, end=None):\n",
    "        return super(MaxSegmentTree, self).reduce(start, end)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearSchedule:\n",
    "    def __init__(self, schedule_timestamps, final_p, initial_p=1.0):\n",
    "        self.schedule_timestamps = schedule_timestamps\n",
    "        self.final_p = final_p\n",
    "        self.initial_p = initial_p\n",
    "\n",
    "    def value(self, t):\n",
    "        fraction = min(1.0, float(t) / self.schedule_timestamps)\n",
    "        return self.initial_p + fraction * (self.final_p - self.initial_p)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_size = 20000\n",
    "memory = PrioritizedReplayBuffer(memory_size, 0.6)\n",
    "\n",
    "possible_actions = np.identity(action_size, dtype=int).tolist()\n",
    "\n",
    "env_info = env.reset()[brain_name]\n",
    "state = env_info.vector_observations[0]\n",
    "\n",
    "pretrain_length = 20000\n",
    "for i in range(pretrain_length):\n",
    "    action = np.random.randint(action_size)\n",
    "    env_info = env.step(action)[brain_name]\n",
    "    reward = env_info.rewards[0]\n",
    "    done = env_info.local_done[0]\n",
    "\n",
    "    if done:\n",
    "        next_state = np.zeros(state.shape)\n",
    "    else:\n",
    "        next_state = env_info.vector_observations[0]\n",
    "\n",
    "    experience = state, action, reward, next_state, done\n",
    "    memory.add(*experience)\n",
    "\n",
    "    if done:\n",
    "        env_info = env.reset()[brain_name]\n",
    "        state = env_info.vector_observations[0]\n",
    "    else:\n",
    "        state = next_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0909 19:18:12.990290 140205682263872 lazy_loader.py:50] \n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "W0909 19:18:12.990820 140205682263872 deprecation.py:323] From <ipython-input-7-388b859a4327>:20: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "W0909 19:18:13.284495 140205682263872 deprecation.py:506] From /home/kgdev/anaconda3/envs/drlnd/lib/python3.6/site-packages/tensorflow/python/training/rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "learning_rate = 0.00025\n",
    "\n",
    "DQNetwork = QNetwork([state_size], action_size, learning_rate, name='DQNetwork')\n",
    "\n",
    "TargetNetwork = QNetwork([state_size], action_size, learning_rate, name='TargetNetwork')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_target_graph():\n",
    "    from_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'DQNetwork')\n",
    "    to_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'TargetNetwork')\n",
    "    op_holder = []\n",
    "    for from_var, to_var in zip(from_vars, to_vars):\n",
    "        op_holder.append(to_var.assign(from_var))\n",
    "    return op_holder\n",
    "\n",
    "\n",
    "def predict_action(explore_probability, state):\n",
    "    tradeoff = np.random.rand()\n",
    "    if explore_probability > tradeoff:\n",
    "        action = np.random.randint(action_size)\n",
    "    else:\n",
    "        Qs = sess.run(DQNetwork.output, feed_dict={DQNetwork.inputs_: state.reshape((1, *state.shape))})\n",
    "        action = np.argmax(Qs)\n",
    "    return action\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[_DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 268435456, 5297597527349262905), _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_GPU:0, XLA_GPU, 17179869184, 4521961447181447024), _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 11660122383662626222), _DeviceAttributes(/job:localhost/replica:0/task:0/device:GPU:0, GPU, 7690629940, 3708971275919048207)]\n",
      "Episode: 0 Total reward: -1.0\n",
      "Episode: 1 Total reward: -2.0\n",
      "Episode: 2 Total reward: 1.0\n",
      "explore_probability: 0.9505 beta: 0.5\n",
      "Episode: 3 Total reward: -1.0\n",
      "Episode: 4 Total reward: 1.0\n",
      "Episode: 5 Total reward: 1.0\n",
      "explore_probability: 0.901 beta: 0.6\n",
      "Episode: 6 Total reward: -2.0\n",
      "Episode: 7 Total reward: 1.0\n",
      "Episode: 8 Total reward: 3.0\n",
      "Episode: 9 Total reward: 2.0\n",
      "explore_probability: 0.8515 beta: 0.7\n",
      "Episode: 10 Total reward: 0.0\n",
      "Episode: 11 Total reward: -1.0\n",
      "Episode: 12 Total reward: -1.0\n",
      "explore_probability: 0.802 beta: 0.8\n",
      "Episode: 13 Total reward: 1.0\n",
      "Episode: 14 Total reward: 1.0\n",
      "Episode: 15 Total reward: 2.0\n",
      "explore_probability: 0.7525 beta: 0.9\n",
      "Episode: 16 Total reward: 0.0\n",
      "Episode: 17 Total reward: -1.0\n",
      "Episode: 18 Total reward: 1.0\n",
      "Episode: 19 Total reward: 2.0\n",
      "explore_probability: 0.7030000000000001 beta: 1.0\n",
      "Episode: 20 Total reward: -3.0\n",
      "Episode: 21 Total reward: 0.0\n",
      "Episode: 22 Total reward: -2.0\n",
      "explore_probability: 0.6535 beta: 1.0\n",
      "Episode: 23 Total reward: 0.0\n",
      "Episode: 24 Total reward: 1.0\n",
      "Episode: 25 Total reward: -1.0\n",
      "explore_probability: 0.604 beta: 1.0\n",
      "Episode: 26 Total reward: -1.0\n",
      "Episode: 27 Total reward: -1.0\n",
      "Episode: 28 Total reward: 0.0\n",
      "Episode: 29 Total reward: 2.0\n",
      "explore_probability: 0.5545 beta: 1.0\n",
      "Episode: 30 Total reward: 1.0\n",
      "Episode: 31 Total reward: -1.0\n",
      "Episode: 32 Total reward: 0.0\n",
      "explore_probability: 0.505 beta: 1.0\n",
      "Model updated\n",
      "Episode: 33 Total reward: 0.0\n",
      "Episode: 34 Total reward: 0.0\n",
      "Episode: 35 Total reward: 5.0\n",
      "explore_probability: 0.4555 beta: 1.0\n",
      "Episode: 36 Total reward: 0.0\n",
      "Episode: 37 Total reward: -3.0\n",
      "Episode: 38 Total reward: 0.0\n",
      "Episode: 39 Total reward: -2.0\n",
      "explore_probability: 0.406 beta: 1.0\n",
      "Episode: 40 Total reward: -1.0\n",
      "Episode: 41 Total reward: 0.0\n",
      "Episode: 42 Total reward: 1.0\n",
      "explore_probability: 0.35650000000000004 beta: 1.0\n",
      "Episode: 43 Total reward: -1.0\n",
      "Episode: 44 Total reward: -1.0\n",
      "Episode: 45 Total reward: 0.0\n",
      "explore_probability: 0.30700000000000005 beta: 1.0\n",
      "Episode: 46 Total reward: 0.0\n",
      "Episode: 47 Total reward: 0.0\n",
      "Episode: 48 Total reward: -2.0\n",
      "Episode: 49 Total reward: 0.0\n",
      "explore_probability: 0.25750000000000006 beta: 1.0\n",
      "Episode: 50 Total reward: 0.0\n",
      "Episode: 51 Total reward: 2.0\n",
      "Episode: 52 Total reward: 2.0\n",
      "explore_probability: 0.20799999999999996 beta: 1.0\n",
      "Episode: 53 Total reward: -2.0\n",
      "Episode: 54 Total reward: 0.0\n",
      "Episode: 55 Total reward: 0.0\n",
      "explore_probability: 0.15849999999999997 beta: 1.0\n",
      "Episode: 56 Total reward: 0.0\n",
      "Episode: 57 Total reward: -3.0\n",
      "Episode: 58 Total reward: 1.0\n",
      "Episode: 59 Total reward: -4.0\n",
      "explore_probability: 0.10899999999999999 beta: 1.0\n",
      "Episode: 60 Total reward: 0.0\n",
      "Episode: 61 Total reward: 0.0\n",
      "Episode: 62 Total reward: 0.0\n",
      "explore_probability: 0.0595 beta: 1.0\n",
      "Episode: 63 Total reward: -1.0\n",
      "Episode: 64 Total reward: -1.0\n",
      "Episode: 65 Total reward: 3.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Model updated\n",
      "Episode: 66 Total reward: 1.0\n",
      "Episode: 67 Total reward: 1.0\n",
      "Episode: 68 Total reward: -2.0\n",
      "Episode: 69 Total reward: -1.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 70 Total reward: 1.0\n",
      "Episode: 71 Total reward: -2.0\n",
      "Episode: 72 Total reward: 3.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 73 Total reward: -2.0\n",
      "Episode: 74 Total reward: 0.0\n",
      "Episode: 75 Total reward: 2.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 76 Total reward: 0.0\n",
      "Episode: 77 Total reward: 1.0\n",
      "Episode: 78 Total reward: 0.0\n",
      "Episode: 79 Total reward: -2.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 80 Total reward: -1.0\n",
      "Episode: 81 Total reward: 1.0\n",
      "Episode: 82 Total reward: -1.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 83 Total reward: 2.0\n",
      "Episode: 84 Total reward: -2.0\n",
      "Episode: 85 Total reward: 1.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 86 Total reward: -1.0\n",
      "Episode: 87 Total reward: 0.0\n",
      "Episode: 88 Total reward: 1.0\n",
      "Episode: 89 Total reward: 0.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 90 Total reward: -3.0\n",
      "Episode: 91 Total reward: 0.0\n",
      "Episode: 92 Total reward: 0.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 93 Total reward: 2.0\n",
      "Episode: 94 Total reward: -1.0\n",
      "Episode: 95 Total reward: 1.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 96 Total reward: -2.0\n",
      "Episode: 97 Total reward: 1.0\n",
      "Episode: 98 Total reward: 1.0\n",
      "Episode: 99 Total reward: -1.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "loss: 0.014611645601689816\n",
      "Model updated\n",
      "Episode: 100 Total reward: 1.0\n",
      "Episode: 101 Total reward: 1.0\n",
      "Episode: 102 Total reward: 1.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 103 Total reward: -1.0\n",
      "Episode: 104 Total reward: 1.0\n",
      "Episode: 105 Total reward: 0.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 106 Total reward: 2.0\n",
      "Episode: 107 Total reward: 2.0\n",
      "Episode: 108 Total reward: -2.0\n",
      "Episode: 109 Total reward: 2.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 110 Total reward: 2.0\n",
      "Episode: 111 Total reward: 2.0\n",
      "Episode: 112 Total reward: 0.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 113 Total reward: 5.0\n",
      "Episode: 114 Total reward: 2.0\n",
      "Episode: 115 Total reward: 2.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 116 Total reward: 1.0\n",
      "Episode: 117 Total reward: 3.0\n",
      "Episode: 118 Total reward: 2.0\n",
      "Episode: 119 Total reward: 2.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 120 Total reward: 3.0\n",
      "Episode: 121 Total reward: 3.0\n",
      "Episode: 122 Total reward: 0.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 123 Total reward: 1.0\n",
      "Episode: 124 Total reward: 2.0\n",
      "Episode: 125 Total reward: 2.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 126 Total reward: 0.0\n",
      "Episode: 127 Total reward: 0.0\n",
      "Episode: 128 Total reward: 0.0\n",
      "Episode: 129 Total reward: 0.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 130 Total reward: 0.0\n",
      "Episode: 131 Total reward: 1.0\n",
      "Episode: 132 Total reward: -2.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Model updated\n",
      "Episode: 133 Total reward: 1.0\n",
      "Episode: 134 Total reward: 6.0\n",
      "Episode: 135 Total reward: 4.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 136 Total reward: 3.0\n",
      "Episode: 137 Total reward: 7.0\n",
      "Episode: 138 Total reward: 7.0\n",
      "Episode: 139 Total reward: 3.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 140 Total reward: 8.0\n",
      "Episode: 141 Total reward: 0.0\n",
      "Episode: 142 Total reward: 2.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 143 Total reward: 7.0\n",
      "Episode: 144 Total reward: 5.0\n",
      "Episode: 145 Total reward: 2.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 146 Total reward: 4.0\n",
      "Episode: 147 Total reward: 2.0\n",
      "Episode: 148 Total reward: 2.0\n",
      "Episode: 149 Total reward: 8.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 150 Total reward: 4.0\n",
      "Episode: 151 Total reward: 5.0\n",
      "Episode: 152 Total reward: 1.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 153 Total reward: 1.0\n",
      "Episode: 154 Total reward: 5.0\n",
      "Episode: 155 Total reward: 5.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 156 Total reward: 5.0\n",
      "Episode: 157 Total reward: 5.0\n",
      "Episode: 158 Total reward: 3.0\n",
      "Episode: 159 Total reward: 11.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 160 Total reward: 2.0\n",
      "Episode: 161 Total reward: 4.0\n",
      "Episode: 162 Total reward: 4.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 163 Total reward: 2.0\n",
      "Episode: 164 Total reward: 2.0\n",
      "Episode: 165 Total reward: 3.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Model updated\n",
      "Episode: 166 Total reward: 4.0\n",
      "Episode: 167 Total reward: 1.0\n",
      "Episode: 168 Total reward: 4.0\n",
      "Episode: 169 Total reward: 1.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 170 Total reward: 5.0\n",
      "Episode: 171 Total reward: 1.0\n",
      "Episode: 172 Total reward: 2.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 173 Total reward: 4.0\n",
      "Episode: 174 Total reward: 0.0\n",
      "Episode: 175 Total reward: 2.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 176 Total reward: 4.0\n",
      "Episode: 177 Total reward: 3.0\n",
      "Episode: 178 Total reward: -2.0\n",
      "Episode: 179 Total reward: 4.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 180 Total reward: 0.0\n",
      "Episode: 181 Total reward: 3.0\n",
      "Episode: 182 Total reward: 3.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 183 Total reward: -1.0\n",
      "Episode: 184 Total reward: 0.0\n",
      "Episode: 185 Total reward: 1.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 186 Total reward: 6.0\n",
      "Episode: 187 Total reward: 1.0\n",
      "Episode: 188 Total reward: 0.0\n",
      "Episode: 189 Total reward: -1.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 190 Total reward: 6.0\n",
      "Episode: 191 Total reward: 0.0\n",
      "Episode: 192 Total reward: 3.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 193 Total reward: 3.0\n",
      "Episode: 194 Total reward: -4.0\n",
      "Episode: 195 Total reward: 0.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 196 Total reward: 1.0\n",
      "Episode: 197 Total reward: 2.0\n",
      "Episode: 198 Total reward: 2.0\n",
      "Episode: 199 Total reward: 0.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "loss: 0.018552126362919807\n",
      "Model updated\n",
      "Episode: 200 Total reward: 5.0\n",
      "Episode: 201 Total reward: 8.0\n",
      "Episode: 202 Total reward: 4.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 203 Total reward: 3.0\n",
      "Episode: 204 Total reward: 1.0\n",
      "Episode: 205 Total reward: 2.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 206 Total reward: 4.0\n",
      "Episode: 207 Total reward: 5.0\n",
      "Episode: 208 Total reward: 5.0\n",
      "Episode: 209 Total reward: 2.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 210 Total reward: 2.0\n",
      "Episode: 211 Total reward: 3.0\n",
      "Episode: 212 Total reward: 6.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 213 Total reward: 0.0\n",
      "Episode: 214 Total reward: 8.0\n",
      "Episode: 215 Total reward: 9.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 216 Total reward: 3.0\n",
      "Episode: 217 Total reward: 3.0\n",
      "Episode: 218 Total reward: 1.0\n",
      "Episode: 219 Total reward: 7.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 220 Total reward: 4.0\n",
      "Episode: 221 Total reward: 4.0\n",
      "Episode: 222 Total reward: 3.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 223 Total reward: 5.0\n",
      "Episode: 224 Total reward: 1.0\n",
      "Episode: 225 Total reward: 2.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 226 Total reward: 6.0\n",
      "Episode: 227 Total reward: 3.0\n",
      "Episode: 228 Total reward: 10.0\n",
      "Episode: 229 Total reward: 2.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 230 Total reward: 2.0\n",
      "Episode: 231 Total reward: 8.0\n",
      "Episode: 232 Total reward: 3.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Model updated\n",
      "Episode: 233 Total reward: 5.0\n",
      "Episode: 234 Total reward: 9.0\n",
      "Episode: 235 Total reward: 4.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 236 Total reward: 2.0\n",
      "Episode: 237 Total reward: 0.0\n",
      "Episode: 238 Total reward: 8.0\n",
      "Episode: 239 Total reward: 4.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 240 Total reward: 4.0\n",
      "Episode: 241 Total reward: 11.0\n",
      "Episode: 242 Total reward: 3.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 243 Total reward: 3.0\n",
      "Episode: 244 Total reward: 12.0\n",
      "Episode: 245 Total reward: 11.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 246 Total reward: 7.0\n",
      "Episode: 247 Total reward: 6.0\n",
      "Episode: 248 Total reward: 6.0\n",
      "Episode: 249 Total reward: 2.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 250 Total reward: 4.0\n",
      "Episode: 251 Total reward: 7.0\n",
      "Episode: 252 Total reward: 10.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 253 Total reward: 5.0\n",
      "Episode: 254 Total reward: 7.0\n",
      "Episode: 255 Total reward: 13.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 256 Total reward: 2.0\n",
      "Episode: 257 Total reward: 3.0\n",
      "Episode: 258 Total reward: 11.0\n",
      "Episode: 259 Total reward: 4.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 260 Total reward: 3.0\n",
      "Episode: 261 Total reward: 5.0\n",
      "Episode: 262 Total reward: 8.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 263 Total reward: 6.0\n",
      "Episode: 264 Total reward: 2.0\n",
      "Episode: 265 Total reward: 11.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Model updated\n",
      "Episode: 266 Total reward: 8.0\n",
      "Episode: 267 Total reward: 11.0\n",
      "Episode: 268 Total reward: 11.0\n",
      "Episode: 269 Total reward: 6.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 270 Total reward: 4.0\n",
      "Episode: 271 Total reward: 6.0\n",
      "Episode: 272 Total reward: 9.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 273 Total reward: 0.0\n",
      "Episode: 274 Total reward: 4.0\n",
      "Episode: 275 Total reward: 6.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 276 Total reward: 9.0\n",
      "Episode: 277 Total reward: 6.0\n",
      "Episode: 278 Total reward: 14.0\n",
      "Episode: 279 Total reward: 10.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 280 Total reward: 13.0\n",
      "Episode: 281 Total reward: 2.0\n",
      "Episode: 282 Total reward: 9.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 283 Total reward: 7.0\n",
      "Episode: 284 Total reward: 4.0\n",
      "Episode: 285 Total reward: 15.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 286 Total reward: 9.0\n",
      "Episode: 287 Total reward: 14.0\n",
      "Episode: 288 Total reward: 7.0\n",
      "Episode: 289 Total reward: 6.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 290 Total reward: 8.0\n",
      "Episode: 291 Total reward: 12.0\n",
      "Episode: 292 Total reward: 7.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 293 Total reward: 19.0\n",
      "Episode: 294 Total reward: 8.0\n",
      "Episode: 295 Total reward: 12.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 296 Total reward: 9.0\n",
      "Episode: 297 Total reward: 6.0\n",
      "Episode: 298 Total reward: 7.0\n",
      "Episode: 299 Total reward: 5.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "loss: 0.027922434732317924\n",
      "Model updated\n",
      "Episode: 300 Total reward: 8.0\n",
      "Episode: 301 Total reward: 8.0\n",
      "Episode: 302 Total reward: 5.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 303 Total reward: 14.0\n",
      "Episode: 304 Total reward: 7.0\n",
      "Episode: 305 Total reward: 9.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 306 Total reward: 5.0\n",
      "Episode: 307 Total reward: 12.0\n",
      "Episode: 308 Total reward: 7.0\n",
      "Episode: 309 Total reward: 9.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 310 Total reward: 5.0\n",
      "Episode: 311 Total reward: 10.0\n",
      "Episode: 312 Total reward: 11.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 313 Total reward: 2.0\n",
      "Episode: 314 Total reward: 8.0\n",
      "Episode: 315 Total reward: 10.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 316 Total reward: 14.0\n",
      "Episode: 317 Total reward: 7.0\n",
      "Episode: 318 Total reward: 8.0\n",
      "Episode: 319 Total reward: 11.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 320 Total reward: 10.0\n",
      "Episode: 321 Total reward: 7.0\n",
      "Episode: 322 Total reward: 9.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 323 Total reward: 12.0\n",
      "Episode: 324 Total reward: 12.0\n",
      "Episode: 325 Total reward: 7.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 326 Total reward: 3.0\n",
      "Episode: 327 Total reward: 4.0\n",
      "Episode: 328 Total reward: 6.0\n",
      "Episode: 329 Total reward: 16.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 330 Total reward: 4.0\n",
      "Episode: 331 Total reward: 12.0\n",
      "Episode: 332 Total reward: 2.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Model updated\n",
      "Episode: 333 Total reward: 9.0\n",
      "Episode: 334 Total reward: 8.0\n",
      "Episode: 335 Total reward: 11.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 336 Total reward: 6.0\n",
      "Episode: 337 Total reward: 6.0\n",
      "Episode: 338 Total reward: 10.0\n",
      "Episode: 339 Total reward: 4.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 340 Total reward: 11.0\n",
      "Episode: 341 Total reward: 10.0\n",
      "Episode: 342 Total reward: 6.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 343 Total reward: 11.0\n",
      "Episode: 344 Total reward: 13.0\n",
      "Episode: 345 Total reward: 8.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 346 Total reward: 9.0\n",
      "Episode: 347 Total reward: 10.0\n",
      "Episode: 348 Total reward: 5.0\n",
      "Episode: 349 Total reward: 6.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 350 Total reward: 12.0\n",
      "Episode: 351 Total reward: 1.0\n",
      "Episode: 352 Total reward: 12.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 353 Total reward: 5.0\n",
      "Episode: 354 Total reward: 10.0\n",
      "Episode: 355 Total reward: 7.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 356 Total reward: 7.0\n",
      "Episode: 357 Total reward: 7.0\n",
      "Episode: 358 Total reward: 13.0\n",
      "Episode: 359 Total reward: 10.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 360 Total reward: 12.0\n",
      "Episode: 361 Total reward: 10.0\n",
      "Episode: 362 Total reward: 5.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 363 Total reward: 10.0\n",
      "Episode: 364 Total reward: 6.0\n",
      "Episode: 365 Total reward: 6.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Model updated\n",
      "Episode: 366 Total reward: 11.0\n",
      "Episode: 367 Total reward: 0.0\n",
      "Episode: 368 Total reward: 3.0\n",
      "Episode: 369 Total reward: 6.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 370 Total reward: 6.0\n",
      "Episode: 371 Total reward: 6.0\n",
      "Episode: 372 Total reward: 12.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 373 Total reward: 7.0\n",
      "Episode: 374 Total reward: 8.0\n",
      "Episode: 375 Total reward: 6.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 376 Total reward: 2.0\n",
      "Episode: 377 Total reward: 7.0\n",
      "Episode: 378 Total reward: 9.0\n",
      "Episode: 379 Total reward: 10.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 380 Total reward: 14.0\n",
      "Episode: 381 Total reward: 15.0\n",
      "Episode: 382 Total reward: 5.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 383 Total reward: 5.0\n",
      "Episode: 384 Total reward: 10.0\n",
      "Episode: 385 Total reward: 11.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 386 Total reward: 5.0\n",
      "Episode: 387 Total reward: 5.0\n",
      "Episode: 388 Total reward: 3.0\n",
      "Episode: 389 Total reward: 1.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 390 Total reward: 14.0\n",
      "Episode: 391 Total reward: 3.0\n",
      "Episode: 392 Total reward: 4.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 393 Total reward: 3.0\n",
      "Episode: 394 Total reward: 3.0\n",
      "Episode: 395 Total reward: 6.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 396 Total reward: 8.0\n",
      "Episode: 397 Total reward: 8.0\n",
      "Episode: 398 Total reward: 3.0\n",
      "Episode: 399 Total reward: 11.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "loss: 0.023201458156108856\n",
      "Model updated\n",
      "Episode: 400 Total reward: 5.0\n",
      "Episode: 401 Total reward: 6.0\n",
      "Episode: 402 Total reward: 14.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 403 Total reward: 5.0\n",
      "Episode: 404 Total reward: 10.0\n",
      "Episode: 405 Total reward: 3.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 406 Total reward: 3.0\n",
      "Episode: 407 Total reward: 4.0\n",
      "Episode: 408 Total reward: 3.0\n",
      "Episode: 409 Total reward: 8.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 410 Total reward: 11.0\n",
      "Episode: 411 Total reward: 9.0\n",
      "Episode: 412 Total reward: 3.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 413 Total reward: 7.0\n",
      "Episode: 414 Total reward: 10.0\n",
      "Episode: 415 Total reward: 6.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 416 Total reward: 8.0\n",
      "Episode: 417 Total reward: 11.0\n",
      "Episode: 418 Total reward: 6.0\n",
      "Episode: 419 Total reward: 12.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 420 Total reward: 10.0\n",
      "Episode: 421 Total reward: 10.0\n",
      "Episode: 422 Total reward: 3.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 423 Total reward: 5.0\n",
      "Episode: 424 Total reward: 2.0\n",
      "Episode: 425 Total reward: 0.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 426 Total reward: 3.0\n",
      "Episode: 427 Total reward: 5.0\n",
      "Episode: 428 Total reward: 4.0\n",
      "Episode: 429 Total reward: 6.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 430 Total reward: 13.0\n",
      "Episode: 431 Total reward: 10.0\n",
      "Episode: 432 Total reward: 0.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Model updated\n",
      "Episode: 433 Total reward: 11.0\n",
      "Episode: 434 Total reward: 2.0\n",
      "Episode: 435 Total reward: 9.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 436 Total reward: 2.0\n",
      "Episode: 437 Total reward: 20.0\n",
      "Episode: 438 Total reward: 13.0\n",
      "Episode: 439 Total reward: 7.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 440 Total reward: 1.0\n",
      "Episode: 441 Total reward: 7.0\n",
      "Episode: 442 Total reward: 8.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 443 Total reward: 12.0\n",
      "Episode: 444 Total reward: 16.0\n",
      "Episode: 445 Total reward: 10.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 446 Total reward: 3.0\n",
      "Episode: 447 Total reward: 5.0\n",
      "Episode: 448 Total reward: 4.0\n",
      "Episode: 449 Total reward: 15.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 450 Total reward: 12.0\n",
      "Episode: 451 Total reward: 14.0\n",
      "Episode: 452 Total reward: 19.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 453 Total reward: 6.0\n",
      "Episode: 454 Total reward: 11.0\n",
      "Episode: 455 Total reward: 4.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 456 Total reward: 15.0\n",
      "Episode: 457 Total reward: 11.0\n",
      "Episode: 458 Total reward: 9.0\n",
      "Episode: 459 Total reward: 11.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 460 Total reward: 11.0\n",
      "Episode: 461 Total reward: 8.0\n",
      "Episode: 462 Total reward: 5.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 463 Total reward: 13.0\n",
      "Episode: 464 Total reward: 11.0\n",
      "Episode: 465 Total reward: 9.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Model updated\n",
      "Episode: 466 Total reward: 4.0\n",
      "Episode: 467 Total reward: 2.0\n",
      "Episode: 468 Total reward: 9.0\n",
      "Episode: 469 Total reward: 5.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 470 Total reward: 15.0\n",
      "Episode: 471 Total reward: 12.0\n",
      "Episode: 472 Total reward: 15.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 473 Total reward: 4.0\n",
      "Episode: 474 Total reward: 8.0\n",
      "Episode: 475 Total reward: 4.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 476 Total reward: 12.0\n",
      "Episode: 477 Total reward: 18.0\n",
      "Episode: 478 Total reward: 10.0\n",
      "Episode: 479 Total reward: 4.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 480 Total reward: 11.0\n",
      "Episode: 481 Total reward: 10.0\n",
      "Episode: 482 Total reward: 3.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 483 Total reward: 2.0\n",
      "Episode: 484 Total reward: 4.0\n",
      "Episode: 485 Total reward: 5.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 486 Total reward: 5.0\n",
      "Episode: 487 Total reward: 9.0\n",
      "Episode: 488 Total reward: 10.0\n",
      "Episode: 489 Total reward: 6.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 490 Total reward: 1.0\n",
      "Episode: 491 Total reward: 10.0\n",
      "Episode: 492 Total reward: 5.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 493 Total reward: 5.0\n",
      "Episode: 494 Total reward: 10.0\n",
      "Episode: 495 Total reward: 2.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 496 Total reward: 14.0\n",
      "Episode: 497 Total reward: 10.0\n",
      "Episode: 498 Total reward: 9.0\n",
      "Episode: 499 Total reward: 4.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "loss: 0.01276242733001709\n",
      "Model updated\n",
      "Episode: 500 Total reward: 10.0\n",
      "Episode: 501 Total reward: 9.0\n",
      "Episode: 502 Total reward: 5.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 503 Total reward: 6.0\n",
      "Episode: 504 Total reward: 12.0\n",
      "Episode: 505 Total reward: 11.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 506 Total reward: 10.0\n",
      "Episode: 507 Total reward: 8.0\n",
      "Episode: 508 Total reward: 12.0\n",
      "Episode: 509 Total reward: 10.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 510 Total reward: 10.0\n",
      "Episode: 511 Total reward: 4.0\n",
      "Episode: 512 Total reward: 14.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 513 Total reward: 8.0\n",
      "Episode: 514 Total reward: 11.0\n",
      "Episode: 515 Total reward: 10.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 516 Total reward: 5.0\n",
      "Episode: 517 Total reward: 10.0\n",
      "Episode: 518 Total reward: 10.0\n",
      "Episode: 519 Total reward: 4.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 520 Total reward: 4.0\n",
      "Episode: 521 Total reward: 6.0\n",
      "Episode: 522 Total reward: 13.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 523 Total reward: 5.0\n",
      "Episode: 524 Total reward: 11.0\n",
      "Episode: 525 Total reward: 7.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 526 Total reward: 8.0\n",
      "Episode: 527 Total reward: 1.0\n",
      "Episode: 528 Total reward: 3.0\n",
      "Episode: 529 Total reward: 10.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 530 Total reward: 12.0\n",
      "Episode: 531 Total reward: 18.0\n",
      "Episode: 532 Total reward: 5.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Model updated\n",
      "Episode: 533 Total reward: 12.0\n",
      "Episode: 534 Total reward: 9.0\n",
      "Episode: 535 Total reward: 8.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 536 Total reward: 8.0\n",
      "Episode: 537 Total reward: 6.0\n",
      "Episode: 538 Total reward: 8.0\n",
      "Episode: 539 Total reward: 14.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 540 Total reward: 12.0\n",
      "Episode: 541 Total reward: 21.0\n",
      "Episode: 542 Total reward: 10.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 543 Total reward: 13.0\n",
      "Episode: 544 Total reward: 14.0\n",
      "Episode: 545 Total reward: 2.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 546 Total reward: 14.0\n",
      "Episode: 547 Total reward: 6.0\n",
      "Episode: 548 Total reward: 14.0\n",
      "Episode: 549 Total reward: 12.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 550 Total reward: 9.0\n",
      "Episode: 551 Total reward: 4.0\n",
      "Episode: 552 Total reward: 6.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 553 Total reward: 9.0\n",
      "Episode: 554 Total reward: 2.0\n",
      "Episode: 555 Total reward: 10.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 556 Total reward: 2.0\n",
      "Episode: 557 Total reward: 5.0\n",
      "Episode: 558 Total reward: 13.0\n",
      "Episode: 559 Total reward: 7.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 560 Total reward: 14.0\n",
      "Episode: 561 Total reward: 10.0\n",
      "Episode: 562 Total reward: 9.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 563 Total reward: 10.0\n",
      "Episode: 564 Total reward: 7.0\n",
      "Episode: 565 Total reward: 1.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Model updated\n",
      "Episode: 566 Total reward: 16.0\n",
      "Episode: 567 Total reward: 9.0\n",
      "Episode: 568 Total reward: 12.0\n",
      "Episode: 569 Total reward: 16.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 570 Total reward: 8.0\n",
      "Episode: 571 Total reward: 18.0\n",
      "Episode: 572 Total reward: 4.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 573 Total reward: 8.0\n",
      "Episode: 574 Total reward: 5.0\n",
      "Episode: 575 Total reward: 7.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 576 Total reward: 8.0\n",
      "Episode: 577 Total reward: 9.0\n",
      "Episode: 578 Total reward: 5.0\n",
      "Episode: 579 Total reward: 4.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 580 Total reward: 7.0\n",
      "Episode: 581 Total reward: 6.0\n",
      "Episode: 582 Total reward: 13.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 583 Total reward: 8.0\n",
      "Episode: 584 Total reward: 12.0\n",
      "Episode: 585 Total reward: 7.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 586 Total reward: 9.0\n",
      "Episode: 587 Total reward: 8.0\n",
      "Episode: 588 Total reward: 1.0\n",
      "Episode: 589 Total reward: 5.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 590 Total reward: 8.0\n",
      "Episode: 591 Total reward: 15.0\n",
      "Episode: 592 Total reward: 10.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 593 Total reward: 6.0\n",
      "Episode: 594 Total reward: 12.0\n",
      "Episode: 595 Total reward: 9.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 596 Total reward: 12.0\n",
      "Episode: 597 Total reward: 13.0\n",
      "Episode: 598 Total reward: 17.0\n",
      "Episode: 599 Total reward: 4.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "loss: 0.014900211244821548\n",
      "Model updated\n",
      "Episode: 600 Total reward: 7.0\n",
      "Episode: 601 Total reward: 14.0\n",
      "Episode: 602 Total reward: 11.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 603 Total reward: 23.0\n",
      "Episode: 604 Total reward: 6.0\n",
      "Episode: 605 Total reward: 6.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 606 Total reward: 6.0\n",
      "Episode: 607 Total reward: 11.0\n",
      "Episode: 608 Total reward: 2.0\n",
      "Episode: 609 Total reward: 6.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 610 Total reward: 4.0\n",
      "Episode: 611 Total reward: 10.0\n",
      "Episode: 612 Total reward: 9.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 613 Total reward: 13.0\n",
      "Episode: 614 Total reward: 2.0\n",
      "Episode: 615 Total reward: 5.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 616 Total reward: 14.0\n",
      "Episode: 617 Total reward: 9.0\n",
      "Episode: 618 Total reward: 5.0\n",
      "Episode: 619 Total reward: 11.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 620 Total reward: 6.0\n",
      "Episode: 621 Total reward: 17.0\n",
      "Episode: 622 Total reward: 12.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 623 Total reward: 2.0\n",
      "Episode: 624 Total reward: 7.0\n",
      "Episode: 625 Total reward: 19.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 626 Total reward: 7.0\n",
      "Episode: 627 Total reward: 5.0\n",
      "Episode: 628 Total reward: 15.0\n",
      "Episode: 629 Total reward: 13.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 630 Total reward: 7.0\n",
      "Episode: 631 Total reward: 13.0\n",
      "Episode: 632 Total reward: 2.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Model updated\n",
      "Episode: 633 Total reward: 8.0\n",
      "Episode: 634 Total reward: 2.0\n",
      "Episode: 635 Total reward: 12.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 636 Total reward: 12.0\n",
      "Episode: 637 Total reward: 9.0\n",
      "Episode: 638 Total reward: 1.0\n",
      "Episode: 639 Total reward: 7.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 640 Total reward: 11.0\n",
      "Episode: 641 Total reward: 5.0\n",
      "Episode: 642 Total reward: 12.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 643 Total reward: 18.0\n",
      "Episode: 644 Total reward: 7.0\n",
      "Episode: 645 Total reward: 12.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 646 Total reward: 13.0\n",
      "Episode: 647 Total reward: 16.0\n",
      "Episode: 648 Total reward: 11.0\n",
      "Episode: 649 Total reward: 9.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 650 Total reward: 12.0\n",
      "Episode: 651 Total reward: 10.0\n",
      "Episode: 652 Total reward: 14.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 653 Total reward: 6.0\n",
      "Episode: 654 Total reward: 6.0\n",
      "Episode: 655 Total reward: 9.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 656 Total reward: 13.0\n",
      "Episode: 657 Total reward: 16.0\n",
      "Episode: 658 Total reward: 3.0\n",
      "Episode: 659 Total reward: 15.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 660 Total reward: 16.0\n",
      "Episode: 661 Total reward: 10.0\n",
      "Episode: 662 Total reward: 20.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 663 Total reward: 6.0\n",
      "Episode: 664 Total reward: 18.0\n",
      "Episode: 665 Total reward: 11.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Model updated\n",
      "Episode: 666 Total reward: 10.0\n",
      "Episode: 667 Total reward: 20.0\n",
      "Episode: 668 Total reward: 1.0\n",
      "Episode: 669 Total reward: 15.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 670 Total reward: 9.0\n",
      "Episode: 671 Total reward: 11.0\n",
      "Episode: 672 Total reward: 10.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 673 Total reward: 4.0\n",
      "Episode: 674 Total reward: 4.0\n",
      "Episode: 675 Total reward: 3.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 676 Total reward: 11.0\n",
      "Episode: 677 Total reward: -1.0\n",
      "Episode: 678 Total reward: 2.0\n",
      "Episode: 679 Total reward: 12.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 680 Total reward: 12.0\n",
      "Episode: 681 Total reward: 10.0\n",
      "Episode: 682 Total reward: 17.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 683 Total reward: 14.0\n",
      "Episode: 684 Total reward: 14.0\n",
      "Episode: 685 Total reward: 6.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 686 Total reward: 1.0\n",
      "Episode: 687 Total reward: 11.0\n",
      "Episode: 688 Total reward: 10.0\n",
      "Episode: 689 Total reward: 8.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 690 Total reward: 5.0\n",
      "Episode: 691 Total reward: 14.0\n",
      "Episode: 692 Total reward: 6.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 693 Total reward: 5.0\n",
      "Episode: 694 Total reward: 8.0\n",
      "Episode: 695 Total reward: 11.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 696 Total reward: 7.0\n",
      "Episode: 697 Total reward: 3.0\n",
      "Episode: 698 Total reward: 13.0\n",
      "Episode: 699 Total reward: 6.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "loss: 0.012354614213109016\n",
      "Model updated\n",
      "Episode: 700 Total reward: 9.0\n",
      "Episode: 701 Total reward: 17.0\n",
      "Episode: 702 Total reward: 4.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 703 Total reward: 10.0\n",
      "Episode: 704 Total reward: 5.0\n",
      "Episode: 705 Total reward: 7.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 706 Total reward: 5.0\n",
      "Episode: 707 Total reward: 4.0\n",
      "Episode: 708 Total reward: 11.0\n",
      "Episode: 709 Total reward: 10.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 710 Total reward: 9.0\n",
      "Episode: 711 Total reward: 1.0\n",
      "Episode: 712 Total reward: 12.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 713 Total reward: 6.0\n",
      "Episode: 714 Total reward: 0.0\n",
      "Episode: 715 Total reward: 11.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 716 Total reward: 12.0\n",
      "Episode: 717 Total reward: 12.0\n",
      "Episode: 718 Total reward: 9.0\n",
      "Episode: 719 Total reward: 10.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 720 Total reward: 6.0\n",
      "Episode: 721 Total reward: 16.0\n",
      "Episode: 722 Total reward: 18.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 723 Total reward: 17.0\n",
      "Episode: 724 Total reward: 15.0\n",
      "Episode: 725 Total reward: 15.0\n",
      "explore_probability: 0.010000000000000009 beta: 1.0\n",
      "Episode: 726 Total reward: 9.0\n"
     ]
    }
   ],
   "source": [
    "total_episodes = 5000\n",
    "max_steps = 5000\n",
    "batch_size = 64\n",
    "max_tau = 10000\n",
    "gamma = 0.95\n",
    "\n",
    "with tf.Session(config=tf.ConfigProto()) as sess:\n",
    "#    sess = tf_debug.LocalCLIDebugWrapperSession(sess)\n",
    "    print(sess.list_devices())\n",
    "\n",
    "    writer = tf.summary.FileWriter('tensorboard/1', sess.graph)\n",
    "    tf.summary.scalar('loss', DQNetwork.loss)\n",
    "    write_op = tf.summary.merge_all()\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    decay_step = 0\n",
    "    tau = 0\n",
    "    \n",
    "    explore_schedule = LinearSchedule(20000, 0.01)\n",
    "    beta_schedule = LinearSchedule(6000, 1, 0.4)\n",
    "\n",
    "    update_target = update_target_graph()\n",
    "    sess.run(update_target)\n",
    "\n",
    "    for episode in range(total_episodes):\n",
    "        step = 0\n",
    "        total_rewards = 0\n",
    "\n",
    "        env_info = env.reset()[brain_name]\n",
    "        state = env_info.vector_observations[0]\n",
    "\n",
    "        while step < max_steps:\n",
    "            step += 1\n",
    "            decay_step += 1\n",
    "            tau += 1\n",
    "    \n",
    "            explore_probability = explore_schedule.value(decay_step)\n",
    "            action = predict_action(explore_probability, state)\n",
    "\n",
    "            env_info = env.step(action)[brain_name]\n",
    "            reward = env_info.rewards[0]\n",
    "            total_rewards += reward\n",
    "            done = env_info.local_done[0]\n",
    "\n",
    "            if done:\n",
    "                next_state = np.zeros(state.shape)\n",
    "                step = max_steps\n",
    "            else:\n",
    "                next_state = env_info.vector_observations[0]\n",
    "\n",
    "            experience = state, action, reward, next_state, done\n",
    "            memory.add(*experience)\n",
    "\n",
    "            if done:\n",
    "                print('Episode: {}'.format(episode),\n",
    "                      'Total reward: {}'.format(total_rewards))\n",
    "            else:\n",
    "                state = next_state\n",
    "\n",
    "            beta = beta_schedule.value(decay_step)\n",
    "            \n",
    "            if decay_step % 1000 == 0:\n",
    "                print(\n",
    "                    'explore_probability: {}'.format(explore_probability),\n",
    "                    'beta: {}'.format(beta),\n",
    "                )\n",
    "            \n",
    "            states, actions, rewards, next_states, dones, weights, idxes = memory.sample(batch_size, beta)\n",
    "            actions = [possible_actions[action] for action in actions]\n",
    "            weights = [[weight] for weight in weights]\n",
    "\n",
    "            q_next_states = sess.run(DQNetwork.output, feed_dict={DQNetwork.inputs_: next_states})\n",
    "            q_target_next_states = sess.run(TargetNetwork.output, feed_dict={TargetNetwork.inputs_: next_states})\n",
    "\n",
    "            target_Q = []\n",
    "            for i in range(0, len(states)):\n",
    "                done = dones[i]\n",
    "                if done:\n",
    "                    target_Q.append(rewards[i])\n",
    "                else:\n",
    "                    action = np.argmax(q_next_states[i])\n",
    "                    target = rewards[i] + gamma * q_target_next_states[i][action]\n",
    "                    target_Q.append(target)\n",
    "\n",
    "            _, summary, loss, absolute_errors = sess.run(\n",
    "                [DQNetwork.optimizer, write_op, DQNetwork.loss, DQNetwork.absolute_errors],\n",
    "                feed_dict={\n",
    "                    DQNetwork.inputs_: states,\n",
    "                    DQNetwork.target_Q: target_Q,\n",
    "                    DQNetwork.actions_: actions,\n",
    "                    DQNetwork.IS_weights_: weights\n",
    "                })\n",
    "\n",
    "            memory.update_priorities(\n",
    "                idxes,\n",
    "                absolute_errors\n",
    "            )\n",
    "\n",
    "            writer.add_summary(summary, episode)\n",
    "            writer.flush()\n",
    "\n",
    "            if tau > max_tau:\n",
    "                sess.run(update_target)\n",
    "                tau = 0\n",
    "                print('Model updated')\n",
    "\n",
    "        if episode % 100 == 99:\n",
    "            print('loss: {}'.format(loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
